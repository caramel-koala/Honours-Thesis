\section{Parallel Merge Performance}
In order to calculate the efficiency of the parallelised section of the merge algorithm, the cell merge algorithm was timed from the moment the parallel and sequential code diverged to the moment at which they become the same again to output their results. The cell merge for both sequential and parallel merges was tested using the same set of data sources, the size of the data source varied with each iteration having 50, 100, 300, 500, 800, and 1000 sources being used to tessellate and then merge. The merge halting criteria is the error threshold defined by Equation \ref{des:eq:maxerr} in Section \ref{des:sec:merge}. The resulting times for the algorithm to generate the merges can be seen in Figure \ref{res:fig:cvg} with a base 10 logarithmic graph of the results in Figure \ref{res:fig:cvg_1og}.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Images/result_cvg.png}
\caption{A comparison of the computation time of a sequential and parallel execution of the cell merge algorithm.}
\label{res:fig:cvg}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Images/result_cvg_log.png}
\caption{A logarithmic scale graph of the values depicted in Figure \ref{res:fig:cvg}.}
\label{res:fig:cvg_1og}
\end{figure}
Observing these results it is clear that the GPU outperforms the CPU with its execution time for a higher number of sources. However, for a low number of sources, the overhead costs associated with the set up process before merging can begin on the GPU make it slower than the CPU. Figure \ref{res:fig:speed} shows the speed-up of the parallel merge as compared to that of the sequential. The speed-up is calculated as $speedup = \frac{t_{sequential}}{t_{parallel}}$.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Images/result_speed.png}
\caption{Speed-up of the parallel process using the times generated by Figure \ref{res:fig:cvg}.}
\label{res:fig:speed}
\end{figure}
From this, it can be seen that the speed-up increases as the number of sources increases. With 1000 sources, a speed-up of 39.96x is obtained. Given that the average speed-up of a parallel implementation of a process, as stated by \citep{lee2010debunking}, is within an order of magnitude of the sequential process, a speed-up of 39.96x can be seen as a great improvement in computation time. 
\\
\\
In order to better analyse the speed-up and provide better insight as to how the parallel implementation operates and what makes up the speeds obtained, the times for each step of the parallel process must be analysed. Figure \ref{res:fig:prof} shows the time taken for each step of the process to execute for a given number of sources.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Images/result_profile.png}
\caption{Breakdown of the time taken for each subprocess in the parallel merge process.}
\label{res:fig:prof}
\end{figure}
Figure \ref{res:fig:prof} shows that, for merges with a fewer number sources, the transfer time to the GPU is relatively constant, while the merge iteration time and data reshape time scale up appropriately, this constant initial transfer cost is therefore what leads to the sequential process outperforming the parallel for smaller sets of sources. For larger sets of sources, the merge iteration and data reshaping time increases at a much higher rate than that of the data transfer.  This again reinforcing the parallel implementation as the preferred operation for larger sets of sources.
\\
\\
While the speed-up of the parallel implementation of the cell merge algorithm is noteworthy, it should also be stated that, even with further optimisation, it will only increase to a theoretical limit. Summarising what was stated by \citep{amdahl1967validity}, we find that the overall speed-up can be calculated as:
\begin{equation}
	speedup = \frac{1}{(1-p)+\frac{p}{s}},
\end{equation}
where $p$ is the percentage of the overall process which can be parallelised and $s$ the speed up of the parallelised portion alone. As with all parallel systems, the cell merge is therefore limited to a maximum speed-up.
\\
\\
The leading limitation of the GPU is one of memory. Large arrays are created to store the relational data on the GPU. For $n$ sources, it can be seen that the memory requirements to compute the merge is on the order of $O(n^2)$ since the \texttt{d\_related} and \texttt{d\_sources} arrays are $n \times n$ and $n \times n \times 3$, respectively where each source is initially used as a centre. Therefore, given the the NVIDIA GTX 750 Ti has an internal memory capacity of 2 GB, it was found that the maximum number of sources which could be merged on the GPU was restricted to around 7500. This is below the average number of sources for an image generated by a radio telescope which is generally on the order of ~10000 sources.