\section{Data Restructuring}
The Voronoi data structure, as described in the previous chapter, consists of centres (Section \ref{sec:design:source}) and lines (Section \ref{sec:des:struct}) linked together by relational centre-line tuples stored in the centre's  list of related centres. While this structure is relatively simple for a sequential CPU system to traverse, GPUs are designed to make use of sequential data readings and therefore traversing a large structure of pointers would be difficult for the GPU to do. Additional justification for restructuring the data is the fact that a structure of values with pointers pointing to different locations in memory can be difficult to transfer to a GPU which is designed to operate on basic data types. Thus the first step in parallelising the merge operation was to obtain a simplified representation of the necessary data.
\subsection{Data Selection}
It was noted that not all of the data from the Voronoi structure would be needed. In order to identify the optimal merge, the following data would be required:
\begin{enumerate}
\item The location of the centre of the cell.
\item The intensity of the cell.
\item The error of the cell.
\item The neighbouring cells of the cell.
\item The sources located in the cell.
\end{enumerate}
It was therefore determined that three multidimensional arrays would be created to store the necessary data to obtain the best possible merge. Given $n$ centres, the required arrays are defined as:
\begin{enumerate}
\item An $n \times 4$ array, named \textit{centres}, of 32 bit floats to store the position, intensity and error of the cell.
\item An $n \times n$ array of integers storing the indexes of the cells to which the cell is related. The size of the array is large to allow space for cells with many neighbours and allow for additions of neighbours through merges. This array is named \textit{related}.
\item Given $s$ initial sources, an $n \times s \times 3$ array of 32 bit floats to hold the positions and intensities of the sources in a cell. This array is also set to be large enough to allow for a single cell to hold all the sources should the error threshold be set extremely high allowing the merge to converge to a single cell. This array is named \textit{sources}.
\end{enumerate}
The creation of these arrays can be seen in Listing \ref{gpu:list:array}

\begin{lstlisting}[language=Python, label=gpu:list:array]
import numpy as np

centre = np.zeros((len(points),4),dtype='float32')
related = np.zeros((len(points),len(points)),dtype='int32')
sources = np.zeros((len(points),numobj,3),dtype='float32')
\end{lstlisting}
\captionof{lstlisting}{Array reconstruction for transfer to the GPU.}
\subsection{Data Transfer}
The three arrays are defined and populated on the CPU but processed on the GPU. While it is possible to leave the data on the CPU and have Numba dynamically transfer the data between the CPU and GPU with each iteration of the merge test kernel call, since the data is minimally altered with each iteration, it is more feasible to transfer the data to the GPU once and alter only some of the data on the GPU after each iteration of the merge process.
\\
\\
Data is transferred from the host (CPU) to the device (GPU) using Numba's \textit{cuda.to\_device()} function, an example of which can be seen in Listing \ref{gpu:list:transfer}.
\begin{lstlisting}[language=Python, label=gpu:list:transfer]
from numba import cuda
...
d_centre = cuda.to_device(centre)
d_related = cuda.to_device(related)
d_sources = cuda.to_device(sources)
\end{lstlisting}
\captionof{lstlisting}{Transferring arrays to GPU.}