\section{Summary}\label{sum}
In retrospect we have discussed many of the technical and theoretical aspects of the algorithm which needs to be developed.
\\
We began by looking at radio astronomy and how radio telescopes are designed as parabolic arcs. We looked at how an array of radio antenna can be used for radio interferometry in order to detect correlations in radio frequency radiation from extragalactic bodies. We analyzed how the two main types of telescope mounts, Altazimuth and Equatorial, work and why the Altazimuth is preferred even though the Equatorial produces a clearer image. The aperture synthesis of telescopes were discussed and how it uses Fourier transforms to produces the image, we also discussed how the primary beam of the antenna affects the image generated. We then saw how DD-effects are generated from this and how they are corrected for and how the need for an algorithm which finds a good compromise between computationally feasibility and sufficient error reduction.
\\
We then looked over Voronoi diagrams and discussed how power diagrams are their natural extension when weights are incorporated. We discussed Voronoi algorithms, namely the incremental, divide and conquer, and Fortune's algorithm. We saw that the incremental was easy to implement but the most time consuming computationally, the divide and conquer used a recursive method which is faster than the incremental, but harder to properly implement, and Fortune's algorithm uses a spatial transform to generate the tessellation in the same amount of time as the divide and conquer, but with the programming ease of the incremental. We then discussed clustering methods for grouping points in a space. We looked at three algorithms again, namely the k-means, the agglomerative, and the bisecting k-means algorithms. We saw that the k-means was effective, but unpredictable in the time it takes to complete and varied in its result depending on where the initial guesses are made. The agglomerative was the most stable but took the longest to calculate, but we see that this can be corrected by first generating a minimum spanning tree of the points. The bisecting k-means stabilized the run time of the k-means with its fixed recursive runs and also produces more stable clusterings.
\\
Lastly we looked at GPUs, their architecture, and some of the concepts involved. We discussed parallelism and how it has arisen as a computational norm from the issue of frequency CPUs are unable to overcome. We then looked to the history of GPUs from their initial use for pixel generation in gaming to the parallel data processing titans they are today. We then looked, more specifically at the NVIDIA GTX 750 Ti, and how its GM107 architecture has improved to give high performance with low power consumption. We looked at how the warps are used in the streaming multiprocessor to allow more processes to run concurrently which in turn improves the efficiency of the GPU. The GPU programming language, CUDA, was discussed and how it uses threads and blocks in a grid for parallel data processing, and how it incorporates special types memory on the GPU in order to further improve the efficiency of the GPU. Finally we discussed CUDA best practices for optimizing the run time of the code, using asynchronous data transfers, running multiple kernels and making use of the CUDA libraries were some of the improvements discussed.
\\
At each stage we also looked at existing models which do work similar to what will be done in the later paper. We looked at the naive grid method for DD-effect error correction and its shortcomings due to empty blocks and off center optimal points. We then discussed a standard Voronoi model for use in 
%--------------------------------------------------------------------------------------