\section{Software}\label{soft}
%--------------------------------------------------------------------------------------
\subsection{CUDA}\label{soft:sec:cuda}
CUDA is a parallel programming language created by NVIDIA for the purpose of running on their brand of GPU's. CUDA was modeled as a C-like language with some C++ features. It's main feature is the way in which it separates CPU and GPU code. The CPU code is labeled as ``host'' code and the GPU's as ``device'' code. Device code is called by the host through a special case of a method, known as a kernel. The basic structure of a kernel is as follows: 
\\
\\
\texttt{kernel0<<<grid, block>>>(params);}
\\
\\
In this instance \texttt{kernel0} would be the name of the kernel being called, \texttt{grid} is the three dimensional value of the number of blocks to be assigned, \texttt{block} being similar to \texttt{grid} is a three dimensional value of the number of threads needed and \texttt{params} is simply the parameters needed by the kernel to execute (similar to those of a method).
%
\subsubsection{Threads}\label{soft:ssec:thread}
The thread is the smallest processing unit of the GPU. GPU threads are designed to be cheap and lightweight compared to those of a CPU so that it can be easily created, run it's small task and be destroyed to make place for th next thread. Threads are arranged into three dimensional blocks with each thread having a unique 3 dimensional ID within that block, namely an x, y and z ID. Generally the thread ID is used as the means of determining the difference in the task process of each thread.
%
\subsubsection{Blocks}\label{soft:ssec:block}
Each block may have a maximum of 2056 threads in total and 1024 for any single dimension, hence why they are bundled into a larger, three dimensional grid structure. Similarly to threads, blocks have a unique three dimensional ID in the grid. Blocks exist such that each step of the processes execute simultaneously. This is done as, more often than not, blocks exchange data within their threads and if this precaution is not taken, race conditions could ensue to break the code. Each block, when executing, must occupy a whole number of warps (rounded up). This is done as warps are constantly in lock step. Threads within a block share a fast memory, located in the L1 cache of the streaming multiprocessor. This shared memory must be preallocated when the kernel is called as a third parameter within the kernel launch (parameters within the triple angle brackets).
%--------------------------------------------------------------------------------------
\subsection{Python}\label{soft:sec:py}
%--------------------------------------------------------------------------------------